{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amr9/Langchain-big-query/blob/main/langchain%20big-query%20code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OlNpEp-SH3R",
        "outputId": "34b52e1d-fa77-4654-c322-0f1f0638f6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccjGB4HepYXm"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install google-cloud-bigquery\n",
        "!pip install jinja2"
      ],
      "metadata": {
        "id": "GkoblyU7peBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "wBHAQ4EriPGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\""
      ],
      "metadata": {
        "id": "-p5lEItbp-eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from langchain import PromptTemplate, OpenAI, LLMChain\n",
        "from google.cloud import bigquery\n",
        "import json\n",
        "import argparse\n",
        "from google.cloud import bigquery"
      ],
      "metadata": {
        "id": "j4wivm14qqiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPLATE = '''\n",
        "Write a BigQuery SQL that achieves the following.\n",
        "```\n",
        "{{ content }}\n",
        "```\n",
        "The format of the target tables is as follows.\n",
        "```json\n",
        "{{ schema }}\n",
        "```\n",
        "Output the SQL in raw text.\n",
        "    '''"
      ],
      "metadata": {
        "id": "r4oiCdi-py_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_schema(table_name: str) -> str:\n",
        "    client = bigquery.Client()\n",
        "    table = client.get_table(table_name)\n",
        "    project_id = table.project\n",
        "    dataset_id = table.dataset_id\n",
        "    table_id = table.table_id\n",
        "    schema = list(map(lambda x: x.to_api_repr(), table.schema))\n",
        "    return {'project':project_id,'dataset':dataset_id,'table':table_id,'schema':schema}\n"
      ],
      "metadata": {
        "id": "aPgCMOTOp9aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_schemas(table_names: list[str]):\n",
        "    return json.dumps([get_schema(n) for n in table_names])"
      ],
      "metadata": {
        "id": "OOLZ1Dgpq0-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(content: str, table_names: list[str], verbose: bool = False):\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"content\",\"schema\"],\n",
        "        template=TEMPLATE,\n",
        "        template_format='jinja2',\n",
        "    )\n",
        "    llm_chain = LLMChain(\n",
        "        llm=OpenAI(temperature=0,model_name=\"gpt-3.5-turbo\"), \n",
        "        prompt=prompt, \n",
        "        verbose=verbose,\n",
        "    )\n",
        "    return llm_chain.predict(content=content, schema=get_schemas(table_names))"
      ],
      "metadata": {
        "id": "ovdXFNKtq-Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict('print everything from the table',['symmetric-span-384617.langchain_test.people','symmetric-span-384617.langchain_test.telephones']))"
      ],
      "metadata": {
        "id": "TM1oy2wwxUHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT *\n",
        "FROM `table_name`\n",
        "WHERE Freehold_building_size__ft2 > 6000\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "results = client.query(query)\n",
        "\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "fLwtPdhxrTWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Create a BigQuery client object\n",
        "client = bigquery.Client()\n",
        "\n",
        "# List all tables in a dataset\n",
        "dataset_ref = client.dataset('Final_New')\n",
        "tables = client.list_tables(dataset_ref)\n",
        "\n",
        "# Print the table names\n",
        "for table in tables:   \n",
        "    s=client.get_table(table)\n",
        "    fields =s.schema\n",
        "    print(\"\\n\"+table.table_id+\"\\n\")\n",
        "    for fields in fields:\n",
        "      print(fields.name)\n",
        "\n"
      ],
      "metadata": {
        "id": "NlGr2NxKhbAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "r3tKdmRq8eOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader(\"/content/test.txt\")\n",
        "loaded_documents = loader.load()"
      ],
      "metadata": {
        "id": "hkpaxdoD-7FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(loaded_documents))"
      ],
      "metadata": {
        "id": "RJZnWC2G227B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5529fe43-7372-4c4e-ec04-1ba796591d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(loaded_documents)"
      ],
      "metadata": {
        "id": "YDscyYkY-kTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents))"
      ],
      "metadata": {
        "id": "zO1RkFYZC-1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b92a0e5-2dd7-454c-e5b5-223256aaa11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[5]"
      ],
      "metadata": {
        "id": "yNkGE2az3iGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bc928c-5bf9-4b48-8359-19ad464794d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='the prciing cost 1000usd for project and we are a big company specialized in software projects', metadata={'source': '/content/test.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isn2U-c2-mdr",
        "outputId": "e38559c5-5f8f-4a41-f481-1bd30fd41e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "you are an agent that work in a company which have data of the projects that the company did with the technologies that they used ,\n",
        "i want you to find if the following text that i will give you now is similiar like the projects we did if yes you tell him about the project and the technologies that we used ,\n",
        "pricing of the company and small brief about it. the prciing cost 1000usd for project and we are a big company specialized in software projects,\n",
        "if no you tell him about the company and the pricing : the cost of project is about 500 usd and we use alot of technologies . and here is the text that you search about:\n",
        "\n",
        "Text:{query}\n",
        "\n",
        "if you can't answer the question tell him about the company and the pricing of it\n",
        "Context: the prciing cost 1000usd for project and we are a big company specialized in software projects\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gVEAqUa8kTLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "etY3V_Ktbmmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "X5feQ3MRbVz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(query=\"we want to do action detection app the technologies that we may use language like python and machine learning library like tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkJyOnLkbeFk",
        "outputId": "de059599-b797-4bba-e980-46e9db5b8be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "you are an agent that work in a company which have data of the projects that the company did with the technologies that they used ,\n",
            "i want you to find if the following text that i will give you now is similiar like the projects we did if yes you tell him about the project and the technologies that we used ,\n",
            "pricing of the company and small brief about it. the prciing cost 1000usd for project and we are a big company specialized in software projects,\n",
            "if no you tell him about the company and the pricing : the cost of project is about 500 usd and we use alot of technologies . and here is the text that you search about:\n",
            "\n",
            "Text:we want to do action detection app the technologies that we may use language like python and machine learning library like tensorflow\n",
            "\n",
            "if you can't answer the question tell him about the company and the pricing of it\n",
            "Context: the prciing cost 1000usd for project and we are a big company specialized in software projects\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ],
      "metadata": {
        "id": "UetbzukD-oxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"we want to do action detection app\"\n",
        "queryWithTemplate = \"you are an agent that work in a company which have data of the projects that the company did with the technologies that they used , i want you to find if the following text that i will give you now is similiar like the projects we did if yes you tell him about the project and the technologies that we used , pricing of the company and small brief about it. the prciing cost 1000usd for project and we are a big company specialized in software projects, if no you tell him about the company and the pricing : the cost of project is about 500 usd and we use alot of technologies . and here is the text that you search about:\" + query"
      ],
      "metadata": {
        "id": "Kwf1CEDz_joh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0,model_name=\"gpt-3.5-turbo\"), vectorstore.as_retriever(), memory=memory)"
      ],
      "metadata": {
        "id": "Y-VUp-OB-ypF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea47355-56fc-4994-e622-4fe6de8c19d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:687: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa({\"question\": prompt.format(query=\"\"\"I am first on here and  I need my chatbot.\n",
        "I would like a chatbot that uses chatgpt and manychat to be on my business website to communicate with customers, I would like it to have data learning so i can upload content on what i do to teach the bot how to respond better.  Additionally i would like to to have integrations into my email and be able to respond to simple monthly updates.  I would like social media integration or a push to the website to interact with the bot. and ultimately. I have 4 businesses i want to be able to put it on as many businesses as i want. since i am using my credentials for gpt and many chat.\"\"\")})"
      ],
      "metadata": {
        "id": "GrC7OHtkCyYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['answer']"
      ],
      "metadata": {
        "id": "ua-i1gIaEO8r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8d1d9d64-5b16-4c03-8800-6180f81958ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No, I do not have experience developing chatbots using ChatGPT and ManyChat with those specific capabilities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import json\n",
        "\n",
        "def get_all_table_schemas():\n",
        "    # create a BigQuery client object\n",
        "    client = bigquery.Client()\n",
        "\n",
        "    # list all datasets in the project\n",
        "    datasets = client.list_datasets()\n",
        "\n",
        "    # loop through each dataset and list all tables\n",
        "    tables_schemas = {}\n",
        "    for dataset in datasets:\n",
        "        dataset_tables_schemas = {}\n",
        "        dataset_ref = client.dataset(dataset.dataset_id)\n",
        "        tables = client.list_tables(dataset_ref)\n",
        "        for table in tables:\n",
        "            table_ref = dataset_ref.table(table.table_id)\n",
        "            table = client.get_table(table_ref)\n",
        "            schema = list(map(lambda x: x.to_api_repr(), table.schema))\n",
        "            dataset_tables_schemas[table.table_id] = schema\n",
        "        tables_schemas[f\"Project: {table.project} | Dataset: {dataset.dataset_id}\"] = dataset_tables_schemas\n",
        "\n",
        "    return json.dumps(tables_schemas, indent=2)\n",
        "\n",
        "print(get_all_table_schemas())\n"
      ],
      "metadata": {
        "id": "vxpEzKVk_zD3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}